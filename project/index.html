<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="Image Reconstruction as a Tool for Feature Analysis">
  <meta property="og:title" content="Image Reconstruction as a Tool for Feature Analysis" />
  <meta property="og:description"
    content="A novel approach for interpreting vision features via image reconstruction" />
  <meta property="og:url" content="https://fusionbrainlab.github.io/feature_analysis" />
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/images/v1_vs_v2.png" />
  <meta property="og:image:width" content="1200" />
  <meta property="og:image:height" content="630" />


  <meta name="twitter:title" content="Image Reconstruction as a Tool for Feature Analysis">
  <meta name="twitter:description" content="A novel approach for interpreting vision features via image reconstruction">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/v1_vs_v2.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="computer vision, feature analysis, image reconstruction, vision encoders">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Image Reconstruction as a Tool for Feature Analysis</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.png">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>

<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Image Reconstruction as a Tool for Feature Analysis</h1>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <strong>Eduard Allakhverdov</strong><sup>§†</sup>,&nbsp;
              </span>
              <span class="author-block">
                <strong>Dmitrii Tarasov</strong><sup>§</sup>,&nbsp;
              </span>
              <span class="author-block">
                <strong>Elizaveta Goncharova</strong><sup>§</sup>,&nbsp;
              </span>
              <span class="author-block">
                <strong>Andrey Kuznetsov</strong><sup>§</sup>,&nbsp;
              </span>
              <ul>
                <sup>§</sup> AIRI, <sup>†</sup> MIPT
              </ul>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">

                <!-- Github link -->
                <span class="link-block">
                  <a href="https://github.com/FusionBrainLab/feature_analysis" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <!-- TODO: add arXiv link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>
              </div>
            </div>
          </div>
        </div>
  </section>



  <!-- Paper abstract -->
  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              Vision encoders are increasingly used in modern applications, from vision-only models to multimodal
              systems such as vision-language models. Despite their remarkable success, it remains unclear how these
              architectures represent features internally. Here, we propose a novel approach for interpreting vision
              features via image reconstruction. We compare two related model families, SigLIP and SigLIP2, which differ
              only in their training objective, and show that encoders pre-trained on image-based tasks retain
              significantly more image information than those trained on non-image tasks such as contrastive learning.
              We further apply our method to a range of vision encoders, ranking them by the informativeness of their
              feature representations. Finally, we demonstrate that manipulating the feature space yields predictable
              changes in reconstructed images, revealing that orthogonal rotations — rather than spatial transformations
              — control color encoding. Our approach can be applied to any vision encoder, shedding light on the inner
              structure of its feature space.
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End paper abstract -->

  <!-- Прописать явно контрибушны: -->

  <!-- (1) interpretability metric -->
  <!-- Текстовое объяснение -->
  <!-- Базовые результаты: siglip vs siglip2 -->
  <!-- Нужно четко подчеркнуть какие различия между модельками -->
  <!-- И как это влияет на реконструкцию -->

  <div class="columns is-centered has-text-centered">
    <div class="column is-four-fifths">
      <h2 class="title is-3" style="white-space: nowrap;">Reconstruct images from feature space</h2>
      <div class="content has-text-justified">
        <div style="text-align: center;">
          <img src="./static/images/features_reconstruction.drawio.png" alt="features_reconstruction" width="900">
          <p class="caption" style="width: 100%; text-align: center;">
            <b>Figure 1. Image reconstructor training.</b> For pretrained model we train a reconstructor model that
            restores the image from the feature space.
          </p>
          <img src="./static/images/reconstruction_metrics.jpg" alt="reconstruction_metrics" width="900">
          <p class="caption" style="width: 100%; text-align: center;">
            <b>Figure 2. Reconstruction Metrics.</b> We show the results of the reconstruction for SigLip and SigLip2
            for different image resultions.
          </p>
        </div>
      </div>
      <br><br>
    </div>
  </div>

  <!-- (2) Feature-space transformations -->
  <!-- Текстовое объяснение -->
  <!-- Визуализация фреймворка: обобщил оператор в пр-ве картинок и в пр-ве фичей -->
  <!-- Примеры работы с RGB -->
  <!-- Примеры работы с отключением одного канала (ожелтением) -->
  <!-- Примеры Спектра такой м-цы, показать, что только небольшое кол-во каналов меняется -->
  <!--  -->

  <div class="columns is-centered has-text-centered">
    <div class="column is-four-fifths">
      <h2 class="title is-3" style="white-space: nowrap;">Feature-space transformations. Q matrix Calculation and
        Application.</h2>
      <div class="content has-text-justified">
        <div class="columns is-centered">
          <div class="column is-half">
            <img src="./static/images/features_reconstruction_manipulation_train_Q.drawio.png"
              alt="features_reconstruction_manipulation_train_Q" width="100%">
            <p class="caption" style="width: 100%; text-align: center;">
              <b>Figure 3. Feature-space transformations. Q matrix Calculation.</b> We then calculate Q matrix for
              feature-space manupulation.
            </p>
          </div>
          <div class="column is-half">
            <img src="./static/images/features_reconstruction_manipulation_eval_Q.drawio.png"
              alt="features_reconstruction_manipulation_eval_Q" width="100%">
            <p class="caption" style="width: 100%; text-align: center;">
              <b>Figure 4. Feature-space transformations. Q matrix Application.</b> After Q matrix is calculated, we
              apply it to the feature space. For each patch embedding.
            </p>
          </div>
        </div>
      </div>
      <br><br>
    </div>
  </div>


  <div class="columns is-centered has-text-centered">
    <div class="column is-four-fifths">
      <h2 class="title is-3" style="white-space: nowrap;">Feature-space transformations. Color Swap Examples.</h2>
      <div class="content has-text-justified">

        <div style="text-align: center;">
          <img src="./static/images/rb_swap.png" alt="rb_swap" width="900">
          <p class="caption" style="width: 100%; text-align: center;">
            <b>Figure 5. Red-blue channel swap samples.</b>
          </p>
        </div>

        <div style="text-align: center;">
          <img src="./static/images/color_swap_all_eigen_values.png" alt="color_swap_all_eigen_values" width="900">
          <p class="caption" style="width: 100%; text-align: center;">
            <b>Figure 6. Eigenvalues for red-blue channel swap matrix.</b> Majority of eigenvalues are close to 1, which
            means that the transformation is close to an identity matrix. While the other cluster of eigenvalues are
            close to -1, which means that for these channels direction is changed to the opposite.
          </p>
        </div>
      </div>
    </div>
    <br><br>
  </div>
  </div>


  <div class="columns is-centered has-text-centered">
    <div class="column is-four-fifths">
      <h2 class="title is-3" style="white-space: nowrap;">Feature-space transformations. Blue Channel Suppression.</h2>
      <div class="content has-text-justified">

        <div style="text-align: center;">
          <img src="./static/images/b_suppression_all_transformations.png" alt="b_suppression_all_transformations"
            width="900">
          <p class="caption" style="width: 100%; text-align: center;">
            <b>Figure 7. Blue channel suppression samples.</b>
          </p>
        </div>
        <div style="text-align: center;">
          <img src="./static/images/b_suppression_all_eigen_values.png" alt="b_suppression_all_eigen_values"
            width="900">
          <p class="caption" style="width: 100%; text-align: center;">
            <b>Figure 8. Eigenvalues for blue channel suppression matrix.</b>
          </p>
        </div>
      </div>
    </div>
    <br><br>
  </div>
  </div>


  <div class="columns is-centered has-text-centered">
    <div class="column is-four-fifths">
      <h2 class="title is-3" style="white-space: nowrap;">Feature-space transformations. Colorization.</h2>
      <div class="content has-text-justified">

        <div style="text-align: center;">
          <img src="./static/images/colorized_examples.png" alt="colorization_all_transformations" width="900">
          <p class="caption" style="width: 100%; text-align: center;">
            <b>Figure 9. Colorization samples.</b>
          </p>
        </div>
      </div>
    </div>
    <br><br>
  </div>
  </div>



  <!-- TODO: add citation -->
  <!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@article{feature_analysis,
  title={Image Reconstruction as a Tool for Feature Analysis},
  author={Allakhverdov, Eduard and Tarasov, Dmitrii and Goncharova, Elizaveta and Kuznetsov, Andrey},
  journal={arXiv preprint},
  year={2024}
}</code></pre>
    </div>
  </section>
  <!--End BibTex citation -->


  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">

            <p>
              This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template"
                target="_blank">Academic Project Page Template</a> which was adopted from the <a
                href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
              You are free to borrow the source code of this website, we just ask that you link back to this page in the
              footer. <br> This website is licensed under a <a rel="license"
                href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
                Commons Attribution-ShareAlike 4.0 International License</a>.
            </p>

          </div>
        </div>
      </div>
    </div>
  </footer>

  <!-- Statcounter tracking code -->

  <!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

  <!-- End of Statcounter Code -->

</body>

</html>