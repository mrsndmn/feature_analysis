<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="Image Reconstruction as a Tool for Feature Analysis">
  <meta property="og:title" content="Image Reconstruction as a Tool for Feature Analysis" />
  <meta property="og:description"
    content="A novel approach for interpreting vision features via image reconstruction" />
  <meta property="og:url" content="https://fusionbrainlab.github.io/feature_analysis" />
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/images/v1_vs_v2.png" />
  <meta property="og:image:width" content="1200" />
  <meta property="og:image:height" content="630" />


  <meta name="twitter:title" content="Image Reconstruction as a Tool for Feature Analysis">
  <meta name="twitter:description" content="A novel approach for interpreting vision features via image reconstruction">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/v1_vs_v2.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="computer vision, feature analysis, image reconstruction, vision encoders">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Image Reconstruction as a Tool for Feature Analysis</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.png">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>

<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Image Reconstruction as a Tool for Feature Analysis</h1>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <strong>Eduard Allakhverdov</strong><sup>¬ß‚Ä†</sup>,&nbsp;
              </span>
              <span class="author-block">
                <strong>Dmitrii Tarasov</strong><sup>¬ß</sup>,&nbsp;
              </span>
              <span class="author-block">
                <strong>Elizaveta Goncharova</strong><sup>¬ß</sup>,&nbsp;
              </span>
              <span class="author-block">
                <strong>Andrey Kuznetsov</strong><sup>¬ß</sup>,&nbsp;
              </span>
              <ul>
                <sup>¬ß</sup> AIRI, <sup>‚Ä†</sup> MIPT
              </ul>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">

                <!-- Github link -->
                <span class="link-block">
                  <a href="https://github.com/FusionBrainLab/feature_analysis" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <!-- TODO: add arXiv link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>
              </div>
            </div>
          </div>
        </div>
  </section>



  <!-- Paper abstract -->
  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              Vision encoders are increasingly used in modern applications, from vision-only models to multimodal
              systems such as vision-language models. Despite their remarkable success, it remains unclear how these
              architectures represent features internally. Here, we propose a novel approach for interpreting vision
              features via image reconstruction. We compare two related model families, SigLIP and SigLIP2, which differ
              only in their training objective, and show that encoders pre-trained on image-based tasks retain
              significantly more image information than those trained on non-image tasks such as contrastive learning.
              We further apply our method to a range of vision encoders, ranking them by the informativeness of their
              feature representations. Finally, we demonstrate that manipulating the feature space yields predictable
              changes in reconstructed images, revealing that orthogonal rotations ‚Äî rather than spatial transformations
              ‚Äî control color encoding. Our approach can be applied to any vision encoder, shedding light on the inner
              structure of its feature space.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End paper abstract -->

  <!--
  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3 has-text-centered">Overview</h2>
          <div class="content has-text-centered">
            <img src="./static/images/teaser.png" alt="teaser" class="is-rounded">
            <p class="caption has-text-centered mb-6">
              <b>Figure 1. Overview.</b> Our method enables direct interpretation of vision encoder features through image reconstruction, revealing how different architectures internally represent visual information. We demonstrate this by (a) comparing feature informativeness between model families, (b) ranking encoders by their feature representation quality, and (c) showing how controlled feature space manipulations produce predictable image changes.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section> -->

  <!-- Key Contributions -->
  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3 has-text-centered">Key Contributions</h2>
          <div class="content">
            <div class="box">
              <div class="content">
                <h4>üîç Novel Feature Analysis Method</h4>
                <p>We introduce a new approach to interpret vision encoder features through direct image reconstruction, providing insights into how these models internally represent visual information.</p>
              </div>
            </div>
            <div class="box">
              <div class="content">
                <h4>üìä Model Family Comparison</h4>
                <p>We reveal that encoders pre-trained on image-based tasks retain significantly more image information compared to those trained on contrastive learning tasks, demonstrated through our SigLIP vs SigLIP2 analysis.</p>
              </div>
            </div>
            <div class="box">
              <div class="content">
                <h4>üé® Feature Space Control</h4>
                <p>We demonstrate that linear transformations in feature space control color encoding of reconstructed images on <strong>three different tasks</strong>: colorization, red-blue channel swap, and blue channel suppression.</p>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- (1) interpretability metric -->
  <!-- –¢–µ–∫—Å—Ç–æ–≤–æ–µ –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ -->
  <!-- –ë–∞–∑–æ–≤—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã: siglip vs siglip2 -->
  <!-- –ù—É–∂–Ω–æ —á–µ—Ç–∫–æ –ø–æ–¥—á–µ—Ä–∫–Ω—É—Ç—å –∫–∞–∫–∏–µ —Ä–∞–∑–ª–∏—á–∏—è –º–µ–∂–¥—É –º–æ–¥–µ–ª—å–∫–∞–º–∏ -->
  <!-- –ò –∫–∞–∫ —ç—Ç–æ –≤–ª–∏—è–µ—Ç –Ω–∞ —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏—é -->

  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3 has-text-centered">Method</h2>
          
          <!-- Method Overview -->
          <div class="content">
            <h3 class="title is-4">Feature Reconstruction Framework</h3>
            <p class="has-text-justified">
              Our method enables direct interpretation of vision encoder features through image reconstruction. We train a decoder network that learns to reconstruct original images from their feature representations, providing a quantitative measure of feature informativeness.
            </p>
            <div class="has-text-centered">
              <img src="./static/images/features_reconstruction.drawio.png" alt="features_reconstruction" class="is-rounded">
              <p class="caption has-text-centered mb-6">
                <b>Figure 1.</b> Our reconstruction framework trains a decoder to restore images from feature representations, enabling direct assessment of feature informativeness.
              </p>
            </div>
          </div>

          <!-- Comparative Analysis -->
          <div class="content mt-6">
            <h3 class="title is-4">Comparative Analysis: SigLIP vs SigLIP2</h3>
            <p class="has-text-justified">
              We compare two related model families that differ only in their training objective: SigLIP (trained with contrastive learning) and SigLIP2 (trained on image-based tasks). This controlled comparison reveals how training objectives influence feature representations.
            </p>
            <div class="has-text-centered">
              <img src="./static/images/reconstruction_metrics.jpg" alt="reconstruction_metrics" class="is-rounded">
              <p class="caption has-text-centered mb-6">
                <b>Figure 2.</b> Reconstruction quality comparison between SigLIP and SigLIP2 across different image resolutions demonstrates that image-based training leads to more informative feature representations.
              </p>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- Feature Space Analysis -->
  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3 has-text-centered">Feature Space Analysis</h2>

          <!-- Q Matrix Framework -->
          <div class="content">
            <h3 class="title is-4">Q Matrix: A Tool for Feature Manipulation</h3>
            <p class="has-text-justified">
              We introduce the Q matrix framework that enables controlled manipulation of feature representations. This orthogonal transformation matrix is learned to perform specific image manipulations, revealing how visual attributes are encoded in the feature space.
            </p>
            <div class="columns is-centered has-vertical-divider">
              <div class="column is-half">
                <img src="./static/images/features_reconstruction_manipulation_train_Q.drawio.png" alt="features_reconstruction_manipulation_train_Q" class="is-rounded mb-4">
                <p class="caption has-text-centered mb-6">
                  <b>Figure 3.</b> Q matrix calculation process learns the transformation needed for specific image manipulations.
                </p>
              </div>
              <div style="border-left: 4px solid gray;margin: 50px;"></div>
              <div class="column is-half">
                <img src="./static/images/features_reconstruction_manipulation_eval_Q.drawio.png" alt="features_reconstruction_manipulation_eval_Q" class="is-rounded mb-4">
                <p class="caption has-text-centered mb-6">
                  <b>Figure 4.</b> Application of Q matrix to feature embeddings enables controlled image manipulation.
                </p>
              </div>
            </div>
          </div>

          <!-- Color Manipulation Results -->
          <div class="content mt-6">
            <h3 class="title is-4">Color Manipulation Studies</h3>
            <p class="has-text-justified">
              Using simple linear transformations in feature space, we demonstrate precise control over color attributes of reconstructed image. Our color manipulation studies serve as a validation of the feature interpretation hypothesis, supported by an image reconstruction approach.
            </p>

            <!-- Colorization -->
            <h4 class="title is-5 mt-4">Image Colorization</h4>

            <p class="has-text-justified">
              Colorization task - transforming grayscale images to their color counterparts. This problem requires following properties:</p>

              <p>1. <strong>Semantic Requirement</strong>: Successful colorization necessitates that the feature space geometry encodes real-world knowledge about plausible color distributions for objects and scenes.</p>

              <p>2. <strong>Non-algorithmic Nature</strong>: Colorization cannot be achieved through simple pixel-wise transformations but requires understanding of image semantics.
            </p>

            <div class="has-text-centered">
              <img src="./static/images/colorized_examples.png" width="80%" alt="colorization_all_transformations" class="is-rounded">
              <p class="caption has-text-centered mb-6">
                <b>Figure 5.</b> Our method enables controlled colorization through feature space manipulation, demonstrating the structured nature of color encoding.
              </p>
            </div>

            <!-- Color Swap -->
            <h4 class="title is-5 mt-4">Red-Blue Channel Swap</h4>
            <p class="has-text-justified">

              <p>Properties we would expect from Red-Blue Channel Swap operator:</p>
              <ul>
                <li><strong>Orthogonal</strong> ‚Äî the operator should be orthogonal, meaning it should preserve the norm of the vector.</li>
                <li><strong>Self-inverse</strong> ‚Äî double application of Red Blue color swapping is Identity transformation in image space.</li>
              </ul>
              <p><strong>Eigenvalues of the operator</strong> will be close to +1 and -1.</p>
              <p>As we will see further all this properties are somehow preserved even for Linear operator with no strict constraints on this properties.</p>
              <p></p>

              <p>We trained three different operators:</p>
              <ol>
                  <li><strong>Orthogonal self-conjugated</strong> ‚Äî as a Procrustes solution with a long-range projection of the operator onto the space of self-conjugated operators.</li>
                  <li><strong>Orthogonal</strong> ‚Äî as a Procrustes solution.</li>
                  <li><strong>Linear</strong> ‚Äî as a regression problem. <em>(Note that this solution cannot be directly used with the reconstructor, as it fails to preserve vector norms. Since the reconstructor was trained exclusively on normalized vectors, we first normalize the resulting outputs before feeding them to the reconstructor.)</em></li>
              </ol>

              <p>As shown in <strong>Figure 7</strong>, the eigenvalues of all operators cluster along the real axis, indicating they primarily represent either eigenvector preservation (near +1) or inversion (near -1). While small deviations from these ideal values exist ‚Äî revealing noise in the feature space ‚Äî these perturbations remain relatively weak. Consequently, the feature space geometry largely preserves the properties expected from the pixel-space channel permutation operator.</p>

            </p>
            <div class="columns is-centered">
              <div class="column is-half">
                <img src="./static/images/color_swap_all_transformations.png" alt="rb_swap" class="is-rounded">
                <p class="caption has-text-centered mb-6">
                  <b>Figure 6.</b> Red-blue channel swap demonstrates precise control over color channels in feature space.
                </p>
              </div>
              <div class="column is-half">
                <img src="./static/images/color_swap_all_eigen_values.png" alt="color_swap_all_eigen_values" class="is-rounded">
                <p class="caption has-text-centered mb-6">
                  <b>Figure 7.</b> Eigenvalue analysis reveals that color transformations affect only specific feature dimensions while preserving others.
                </p>
              </div>
            </div>

            <!-- Blue Suppression -->
            <h4 class="title is-5 mt-4">Blue Channel Suppression</h4>
            <p class="has-text-justified">

              <p><strong>Blue Channel Suppression operator</strong> will gradually suppress the blue channel of the image multiplying blue channel by some factor less than 1.</p>

              <p>Properties we would expect from <strong>Blue Channel Suppression operator</strong>:</p>
              <ul>
                <li>Asymptotically this operator approaches a projection operator</li>
              </ul>
            </p>
            <p><strong>Eigenvalues of the operator</strong> are either 1 or complex values with magnitude strictly less than 1.</p>
            <p>We emperically observe this properties.</p>

            <div class="columns is-centered">
              <div class="column is-half">
                <img src="./static/images/b_suppression_all_transformations.png" alt="b_suppression_all_transformations" class="is-rounded">
                <p class="caption has-text-centered mb-6">
                  <b>Figure 8.</b> Selective suppression of the blue channel demonstrates fine-grained control over color attributes.
                </p>
              </div>
              <div class="column is-half">
                <img src="./static/images/b_suppression_all_eigen_values.png" alt="b_suppression_all_eigen_values" class="is-rounded">
                <p class="caption has-text-centered mb-6">
                  <b>Figure 9.</b> Eigenvalue distribution for blue suppression shows targeted modification of specific feature dimensions.
                </p>
              </div>
            </div>

          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- Conclusion -->
  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3 has-text-centered">Conclusion</h2>
          <div class="content has-text-justified">
            <p>
              Our work introduces a novel approach to understanding vision encoder features through image reconstruction. We demonstrate that:
            </p>
            <ul>
              <li>Training objectives significantly impact how models internally represent visual information</li>
              <li>Image-based pre-training leads to more informative feature representations compared to contrastive learning</li>
              <li>Color information is encoded through orthogonal rotations in feature space</li>
              <li>Our method provides a general framework for analyzing any vision encoder's feature representations</li>
            </ul>
            <p>
              These findings have important implications for model design and provide new tools for understanding and controlling vision encoder behavior. Our approach opens new avenues for feature analysis and manipulation in vision models.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- TODO: add citation -->
  <!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@article{feature_analysis,
  title={Image Reconstruction as a Tool for Feature Analysis},
  author={Allakhverdov, Eduard and Tarasov, Dmitrii and Goncharova, Elizaveta and Kuznetsov, Andrey},
  journal={arXiv preprint},
  year={2024}
}</code></pre>
    </div>
  </section>
  <!--End BibTex citation -->


  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">

            <p>
              This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template"
                target="_blank">Academic Project Page Template</a> which was adopted from the <a
                href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
              You are free to borrow the source code of this website, we just ask that you link back to this page in the
              footer. <br> This website is licensed under a <a rel="license"
                href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
                Commons Attribution-ShareAlike 4.0 International License</a>.
            </p>

          </div>
        </div>
      </div>
    </div>
  </footer>

  <!-- Statcounter tracking code -->

  <!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

  <!-- End of Statcounter Code -->

  <!-- Add modal HTML structure -->
  <div id="imageModal" class="modal">
    <span class="modal-close">&times;</span>
    <img class="modal-content" id="modalImage">
  </div>

  <script>
    // Get the modal
    const modal = document.getElementById("imageModal");
    const modalImg = document.getElementById("modalImage");
    const closeBtn = document.getElementsByClassName("modal-close")[0];

    // Add click event to all images
    document.querySelectorAll('.image.is-16by9 img').forEach(img => {
      img.onclick = function() {
        modal.style.display = "block";
        modalImg.src = this.src;
      }
    });

    // Close modal when clicking the close button
    closeBtn.onclick = function() {
      modal.style.display = "none";
    }

    // Close modal when clicking outside the image
    window.onclick = function(event) {
      if (event.target == modal) {
        modal.style.display = "none";
      }
    }

    // Close modal with Escape key
    document.addEventListener('keydown', function(event) {
      if (event.key === "Escape") {
        modal.style.display = "none";
      }
    });
  </script>

</body>

</html>